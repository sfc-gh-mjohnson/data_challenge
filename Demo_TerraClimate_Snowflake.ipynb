{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure shared library path\n",
    "# This allows the notebook to use packages installed in the shared directory\n",
    "import sys\n",
    "\n",
    "SITE_SHARED = \"/workspace/site-packages-shared\"\n",
    "\n",
    "if SITE_SHARED not in sys.path:\n",
    "    sys.path.append(SITE_SHARED)\n",
    "    \n",
    "print(f\"✓ Shared library path configured: {SITE_SHARED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terra Climate Demonstration Notebook - Snowflake Edition\n",
    "\n",
    "**Modified for Snowflake Notebook Deployment**\n",
    "\n",
    "This notebook demonstrates how to access the TerraClimate dataset within Snowflake using PyPI-enabled UDFs and Anaconda packages. TerraClimate is a dataset of monthly climate and climatic water balance for global terrestrial surfaces from 1958 to the present.\n",
    "\n",
    "**Key Changes from Local Version:**\n",
    "- Uses Snowflake UDFs with PyPI packages for data acquisition\n",
    "- Leverages Anaconda packages for data processing\n",
    "- Stores outputs to Snowflake stages\n",
    "- Uses Snowpark for data manipulation\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Run `snowflake_setup.sql` to create UDFs and infrastructure\n",
    "2. PyPI repository access granted to your role\n",
    "3. Connected to TERRACLIMATE_DB database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Snowflake libraries\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.functions import col, avg, min as min_, max as max_, count\n",
    "from snowflake.snowpark.types import *\n",
    "\n",
    "# Get the active Snowflake session\n",
    "session = get_active_session()\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Connected to Snowflake\")\n",
    "print(f\"  Current database: {session.get_current_database()}\")\n",
    "print(f\"  Current schema: {session.get_current_schema()}\")\n",
    "print(f\"  Current warehouse: {session.get_current_warehouse()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set context (if not already set)\n",
    "session.sql(\"USE DATABASE TERRACLIMATE_DB\").collect()\n",
    "session.sql(\"USE SCHEMA CLIMATE_DATA\").collect()\n",
    "\n",
    "print(\"✓ Context set to TERRACLIMATE_DB.CLIMATE_DATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Python Dependencies\n",
    "\n",
    "Using Anaconda packages available in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common data science and visualization libraries (from Anaconda)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import jet\n",
    "import json\n",
    "\n",
    "# Configure matplotlib for inline plotting\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"  NumPy version: {np.__version__}\")\n",
    "print(f\"  Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Access TerraClimate Collection Metadata\n",
    "\n",
    "Using UDF that connects to Planetary Computer via PyPI packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the UDF to get metadata\n",
    "metadata_df = session.sql(\"SELECT get_terraclimate_metadata() as metadata\").collect()\n",
    "metadata = json.loads(metadata_df[0]['METADATA'])\n",
    "\n",
    "print(\"TerraClimate Collection Metadata:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ID: {metadata.get('id')}\")\n",
    "print(f\"Title: {metadata.get('title')}\")\n",
    "print(f\"\\nDescription: {metadata.get('description', 'N/A')[:200]}...\")\n",
    "print(f\"\\nLicense: {metadata.get('license')}\")\n",
    "print(f\"\\nSpatial Extent: {metadata.get('spatial_extent')}\")\n",
    "print(f\"Temporal Extent: {metadata.get('temporal_extent')}\")\n",
    "print(f\"\\nAvailable Assets: {metadata.get('assets')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Zarr Asset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Zarr asset information\n",
    "asset_df = session.sql(\"SELECT get_zarr_asset_info() as asset_info\").collect()\n",
    "asset_info = json.loads(asset_df[0]['ASSET_INFO'])\n",
    "\n",
    "print(\"Zarr Asset Information:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Data URL: {asset_info.get('href')}\")\n",
    "print(f\"\\nTitle: {asset_info.get('title')}\")\n",
    "print(f\"\\nDescription: {asset_info.get('description', 'N/A')}\")\n",
    "print(f\"\\nExtra Fields: {json.dumps(asset_info.get('extra_fields', {}), indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Data Access Parameters\n",
    "\n",
    "Define the region and time period of interest (southeastern Australia, 2017-2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for our analysis\n",
    "start_date = \"2017-11-01\"\n",
    "end_date = \"2019-11-01\"\n",
    "\n",
    "# Southeastern Australia bounding box\n",
    "min_lon = 139.94\n",
    "max_lon = 151.48\n",
    "min_lat = -39.74\n",
    "max_lat = -30.92\n",
    "\n",
    "print(\"Analysis Parameters:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Time Period: {start_date} to {end_date}\")\n",
    "print(f\"Region: Southeastern Australia\")\n",
    "print(f\"  Longitude: {min_lon}° to {max_lon}°\")\n",
    "print(f\"  Latitude: {min_lat}° to {max_lat}°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use UDF to prepare data access\n",
    "access_query = f\"\"\"\n",
    "SELECT prepare_terraclimate_access(\n",
    "    '{start_date}',\n",
    "    '{end_date}',\n",
    "    {min_lon},\n",
    "    {max_lon},\n",
    "    {min_lat},\n",
    "    {max_lat}\n",
    ") as access_info\n",
    "\"\"\"\n",
    "\n",
    "access_df = session.sql(access_query).collect()\n",
    "access_info = json.loads(access_df[0]['ACCESS_INFO'])\n",
    "\n",
    "print(\"\\nData Access Information:\")\n",
    "print(json.dumps(access_info, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Description and Available Variables\n",
    "\n",
    "**Available TerraClimate Variables:**\n",
    "\n",
    "| Variable | Description | Units |\n",
    "|----------|-------------|-------|\n",
    "| `aet` | Actual Evapotranspiration | mm |\n",
    "| `def` | Climate Water Deficit | mm |\n",
    "| `pet` | Potential Evapotranspiration | mm |\n",
    "| `ppt` | Precipitation | mm |\n",
    "| `q` | Runoff | mm |\n",
    "| `soil` | Soil Moisture | mm |\n",
    "| `srad` | Downward Surface Shortwave Radiation | W/m² |\n",
    "| `swe` | Snow Water Equivalent | mm |\n",
    "| `tmax` | Maximum Temperature | °C |\n",
    "| `tmin` | Minimum Temperature | °C |\n",
    "| `vap` | Vapor Pressure | kPa |\n",
    "| `vpd` | Vapor Pressure Deficit | kPa |\n",
    "| `ws` | Wind Speed | m/s |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Working with Sample Data\n",
    "\n",
    "**Note:** Direct xarray loading from external URLs in Snowflake notebooks requires network access configuration. For demonstration purposes, we'll work with sample data structure.\n",
    "\n",
    "**Alternative Approach:** Load pre-processed data from Snowflake stages or tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data structure to demonstrate the workflow\n",
    "# In production, this would come from actual TerraClimate data loaded into Snowflake\n",
    "\n",
    "# Generate sample dates\n",
    "dates = pd.date_range(start=start_date, end=end_date, freq='MS')  # Monthly start\n",
    "\n",
    "# Create sample climate data\n",
    "np.random.seed(42)\n",
    "sample_data = pd.DataFrame({\n",
    "    'time': dates,\n",
    "    'tmax': 15 + 10 * np.sin(np.arange(len(dates)) * 2 * np.pi / 12) + np.random.randn(len(dates)) * 2,\n",
    "    'ppt': 50 + 30 * np.cos(np.arange(len(dates)) * 2 * np.pi / 12) + np.random.randn(len(dates)) * 10,\n",
    "    'soil': 150 + 50 * np.cos(np.arange(len(dates)) * 2 * np.pi / 12 + np.pi/2) + np.random.randn(len(dates)) * 20,\n",
    "    'srad': 200 + 50 * np.sin(np.arange(len(dates)) * 2 * np.pi / 12) + np.random.randn(len(dates)) * 15,\n",
    "    'vap': 1.5 + 0.5 * np.sin(np.arange(len(dates)) * 2 * np.pi / 12) + np.random.randn(len(dates)) * 0.2\n",
    "})\n",
    "\n",
    "print(\"Sample Data Structure (mimicking TerraClimate format):\")\n",
    "print(\"=\" * 50)\n",
    "print(sample_data.head(10))\n",
    "print(f\"\\nTotal records: {len(sample_data)}\")\n",
    "print(f\"Date range: {sample_data['time'].min()} to {sample_data['time'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upload Sample Data to Snowflake Table\n",
    "\n",
    "Store the data in Snowflake for analysis\n",
    "\n",
    "**Important Notes:**\n",
    "- **Column names**: Pandas DataFrame columns (lowercase) are converted to uppercase to avoid Snowflake quoted identifier issues\n",
    "- **Date types**: Datetime columns are explicitly cast to DATE/TIMESTAMP types for date functions like DATE_TRUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time column to string format for Snowflake\n",
    "sample_data['time'] = sample_data['time'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Rename columns to uppercase to match Snowflake convention\n",
    "# This prevents quoted identifier issues\n",
    "sample_data.columns = sample_data.columns.str.upper()\n",
    "\n",
    "# Convert to Snowpark DataFrame and save to table\n",
    "snowpark_df = session.create_dataframe(sample_data)\n",
    "\n",
    "# Write to a temporary table for demonstration\n",
    "table_name = \"SAMPLE_CLIMATE_TIME_SERIES\"\n",
    "snowpark_df.write.mode(\"overwrite\").save_as_table(table_name)\n",
    "\n",
    "# Cast the TIME column to proper DATE type\n",
    "cast_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {table_name} AS\n",
    "SELECT \n",
    "    TO_DATE(TIME) as TIME,\n",
    "    TMAX,\n",
    "    PPT,\n",
    "    SOIL,\n",
    "    SRAD,\n",
    "    VAP\n",
    "FROM {table_name}\n",
    "\"\"\"\n",
    "session.sql(cast_query).collect()\n",
    "\n",
    "print(f\"✓ Data uploaded to table: {table_name}\")\n",
    "print(f\"✓ Columns created with uppercase names (TIME, TMAX, PPT, SOIL, SRAD, VAP)\")\n",
    "print(f\"✓ TIME column cast to DATE type\")\n",
    "\n",
    "# Verify the data\n",
    "result = session.table(table_name).count()\n",
    "print(f\"✓ Record count in table: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exploring the Data - Temperature Analysis\n",
    "\n",
    "Plot mean temperature over the region demonstrating seasonal variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data for plotting\n",
    "temp_df = session.table(table_name).select(\"TIME\", \"TMAX\").to_pandas()\n",
    "\n",
    "# Plot temperature time series\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(temp_df['TIME'], temp_df['TMAX'], linewidth=2, color='#FF6B35', marker='o', markersize=4)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Maximum Temperature (°C)', fontsize=12)\n",
    "ax.set_title('Mean Maximum Temperature Over Time - Southeastern Australia', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Temperature Statistics:\")\n",
    "print(f\"  Mean: {temp_df['TMAX'].mean():.2f}°C\")\n",
    "print(f\"  Min: {temp_df['TMAX'].min():.2f}°C\")\n",
    "print(f\"  Max: {temp_df['TMAX'].max():.2f}°C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Precipitation Analysis\n",
    "\n",
    "Plot monthly accumulated precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve precipitation data\n",
    "precip_df = session.table(table_name).select(\"TIME\", \"PPT\").to_pandas()\n",
    "\n",
    "# Plot precipitation time series\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(precip_df['TIME'], precip_df['PPT'], color='#4ECDC4', width=20)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Precipitation (mm)', fontsize=12)\n",
    "ax.set_title('Monthly Accumulated Precipitation - Southeastern Australia', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Precipitation Statistics:\")\n",
    "print(f\"  Mean: {precip_df['PPT'].mean():.2f} mm\")\n",
    "print(f\"  Min: {precip_df['PPT'].min():.2f} mm\")\n",
    "print(f\"  Max: {precip_df['PPT'].max():.2f} mm\")\n",
    "print(f\"  Total: {precip_df['PPT'].sum():.2f} mm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Soil Moisture Analysis\n",
    "\n",
    "Calculate and visualize soil moisture statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate soil moisture statistics using Snowpark\n",
    "soil_stats = session.table(table_name).select(\n",
    "    avg(col(\"SOIL\")).alias(\"avg_soil\"),\n",
    "    min_(col(\"SOIL\")).alias(\"min_soil\"),\n",
    "    max_(col(\"SOIL\")).alias(\"max_soil\"),\n",
    "    count(col(\"SOIL\")).alias(\"count_soil\")\n",
    ").collect()\n",
    "\n",
    "print(\"Soil Moisture Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Mean: {soil_stats[0]['AVG_SOIL']:.2f} mm\")\n",
    "print(f\"  Min: {soil_stats[0]['MIN_SOIL']:.2f} mm\")\n",
    "print(f\"  Max: {soil_stats[0]['MAX_SOIL']:.2f} mm\")\n",
    "print(f\"  Records: {soil_stats[0]['COUNT_SOIL']}\")\n",
    "\n",
    "# Plot soil moisture over time\n",
    "soil_df = session.table(table_name).select(\"TIME\", \"SOIL\").to_pandas()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.plot(soil_df['TIME'], soil_df['SOIL'], linewidth=2, color='#8B4513', marker='s', markersize=4)\n",
    "ax.axhline(y=soil_stats[0]['AVG_SOIL'], color='red', linestyle='--', label=f\"Mean: {soil_stats[0]['AVG_SOIL']:.1f} mm\")\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Soil Moisture (mm)', fontsize=12)\n",
    "ax.set_title('Soil Moisture Over Time - Southeastern Australia', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Calculate Aggregate Statistics Using Snowpark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQL to calculate comprehensive statistics\n",
    "stats_query = f\"\"\"\n",
    "SELECT \n",
    "    'Temperature (°C)' as variable,\n",
    "    ROUND(AVG(TMAX), 2) as mean_value,\n",
    "    ROUND(MIN(TMAX), 2) as min_value,\n",
    "    ROUND(MAX(TMAX), 2) as max_value,\n",
    "    ROUND(STDDEV(TMAX), 2) as std_dev\n",
    "FROM {table_name}\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Precipitation (mm)',\n",
    "    ROUND(AVG(PPT), 2),\n",
    "    ROUND(MIN(PPT), 2),\n",
    "    ROUND(MAX(PPT), 2),\n",
    "    ROUND(STDDEV(PPT), 2)\n",
    "FROM {table_name}\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Soil Moisture (mm)',\n",
    "    ROUND(AVG(SOIL), 2),\n",
    "    ROUND(MIN(SOIL), 2),\n",
    "    ROUND(MAX(SOIL), 2),\n",
    "    ROUND(STDDEV(SOIL), 2)\n",
    "FROM {table_name}\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Solar Radiation (W/m²)',\n",
    "    ROUND(AVG(SRAD), 2),\n",
    "    ROUND(MIN(SRAD), 2),\n",
    "    ROUND(MAX(SRAD), 2),\n",
    "    ROUND(STDDEV(SRAD), 2)\n",
    "FROM {table_name}\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Vapor Pressure (kPa)',\n",
    "    ROUND(AVG(VAP), 2),\n",
    "    ROUND(MIN(VAP), 2),\n",
    "    ROUND(MAX(VAP), 2),\n",
    "    ROUND(STDDEV(VAP), 2)\n",
    "FROM {table_name}\n",
    "\"\"\"\n",
    "\n",
    "stats_df = session.sql(stats_query).to_pandas()\n",
    "print(\"\\nComprehensive Climate Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "print(stats_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Export Results to Snowflake Stage\n",
    "\n",
    "Save processed data to Snowflake stage (replaces local GeoTIFF export)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table with aggregated results\n",
    "summary_table = \"CLIMATE_SUMMARY_RESULTS\"\n",
    "\n",
    "# Note: TIME column is now properly cast as DATE type, so DATE_TRUNC works correctly\n",
    "summary_query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {summary_table} AS\n",
    "SELECT \n",
    "    DATE_TRUNC('MONTH', TIME) as month,\n",
    "    AVG(TMAX) as avg_max_temp,\n",
    "    AVG(PPT) as avg_precipitation,\n",
    "    AVG(SOIL) as avg_soil_moisture,\n",
    "    AVG(SRAD) as avg_solar_radiation,\n",
    "    AVG(VAP) as avg_vapor_pressure\n",
    "FROM {table_name}\n",
    "GROUP BY DATE_TRUNC('MONTH', TIME)\n",
    "ORDER BY month\n",
    "\"\"\"\n",
    "\n",
    "session.sql(summary_query).collect()\n",
    "print(f\"✓ Summary results saved to table: {summary_table}\")\n",
    "\n",
    "# Display the summary\n",
    "summary_result = session.table(summary_table).to_pandas()\n",
    "print(\"\\nMonthly Summary:\")\n",
    "print(summary_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary data to Snowflake stage as Parquet\n",
    "stage_path = \"@TERRACLIMATE_STAGE\"\n",
    "\n",
    "# Export using COPY INTO command\n",
    "export_query = f\"\"\"\n",
    "COPY INTO {stage_path}/summary_results/\n",
    "FROM {summary_table}\n",
    "FILE_FORMAT = (TYPE = PARQUET)\n",
    "OVERWRITE = TRUE\n",
    "HEADER = TRUE\n",
    "\"\"\"\n",
    "\n",
    "result = session.sql(export_query).collect()\n",
    "print(f\"✓ Data exported to stage: {stage_path}/summary_results/\")\n",
    "print(f\"  Rows exported: {result[0][0]}\")\n",
    "\n",
    "# List files in stage\n",
    "list_query = f\"LIST {stage_path}/summary_results/\"\n",
    "files = session.sql(list_query).collect()\n",
    "print(\"\\nFiles in stage:\")\n",
    "for file in files:\n",
    "    print(f\"  - {file['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary and Next Steps\n",
    "\n",
    "### What We Accomplished:\n",
    "✅ Connected to TerraClimate via PyPI-enabled UDFs  \n",
    "✅ Retrieved collection metadata and asset information  \n",
    "✅ Processed climate data using Snowpark and Anaconda packages  \n",
    "✅ Created visualizations of temperature, precipitation, and soil moisture  \n",
    "✅ Calculated comprehensive statistics using SQL  \n",
    "✅ Exported results to Snowflake stages for persistence  \n",
    "\n",
    "### Key Differences from Local Deployment:\n",
    "- **Data Access**: Uses UDFs with PyPI packages instead of direct API calls\n",
    "- **Processing**: Leverages Snowpark for distributed computing\n",
    "- **Storage**: Uses Snowflake stages instead of local files\n",
    "- **Packages**: Combines PyPI (via UDFs) and Anaconda packages\n",
    "\n",
    "### Next Steps for Production Use:\n",
    "1. **Full Data Loading**: Implement complete xarray data loading via UDFs\n",
    "2. **Spatial Processing**: Add support for actual spatial data processing\n",
    "3. **GeoTIFF Export**: Create UDF to generate GeoTIFF files in stages\n",
    "4. **Scheduled Updates**: Set up tasks to refresh data periodically\n",
    "5. **Advanced Analytics**: Integrate with Snowflake ML for climate modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify available Anaconda packages for spatial operations\n",
    "packages_query = \"\"\"\n",
    "SELECT package_name, version \n",
    "FROM information_schema.packages \n",
    "WHERE language = 'python' \n",
    "AND package_name IN ('numpy', 'pandas', 'xarray', 'matplotlib', 'rasterio', 'rioxarray')\n",
    "ORDER BY package_name\n",
    "\"\"\"\n",
    "\n",
    "available_packages = session.sql(packages_query).to_pandas()\n",
    "print(\"Available Anaconda Packages for Spatial Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "print(available_packages.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
